# Default values for libre-chat.
# Declare variables to be passed into your templates.

chatConf:
  # Config for a Question Answering (qa) agent
  # Will answer based on provided documents, and return which docs was used to answer the question
  llm:
    model_type: llama
    model_path: ./models/llama-2-7b-chat.ggmlv3.q3_K_L.bin # We recommend to predownload the files, but you can provide download URLs that will be used if the files are not present
    model_download: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q3_K_L.bin
    temperature: 0.01    # Config how creative (but also potentially wrong) the model can be. 0 is safe, 1 is adventurous
    max_new_tokens: 1024 # Max number of words the LLM can generate

  prompt:
    variables: ["question", "context"]
    template: |
      Use the following pieces of information to answer the user's question.
      If you don't know the answer, just say that you don't know, don't try to make up an answer.

      Context: {context}
      Question: {question}

      Only return the helpful answer below and nothing else.
      Helpful answer:

  vector:
    vector_path: ./vectorstore/db_faiss            # Path to the vectorstore to do QA retrieval
    vector_download: null
    embeddings_path: ./embeddings/all-MiniLM-L6-v2 # Embeddings used to generate the vectors
    # You can also directly use embeddings model from HuggingFace:
    # embeddings_path: sentence-transformers/all-MiniLM-L6-v2
    embeddings_download: https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/all-MiniLM-L6-v2.zip
    documents_path: ./documents # Path to documents to vectorize
    # When vectorizing we split the text up into small, semantically meaningful chunks (often sentences):
    chunk_size: 500             # Maximum size of chunks, in terms of number of characters
    chunk_overlap: 50           # Overlap in characters between chunks
    chain_type: stuff           # Or: map_reduce, reduce, map_rerank. More details: https://docs.langchain.com/docs/components/chains/index_related_chains
    search_type: similarity     # Or: similarity_score_threshold, mmr. More details: https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore
    return_sources_count: 2     # Number of sources to return when generating an answer
    score_threshold: null       # If using the similarity_score_threshold search type. Between 0 and 1

  info:
    title: "Libre Chat"
    version: "0.1.0"
    description: |
      Open source and free question-answering chatbot powered by [LangChain](https://python.langchain.com) and [Llama 2](https://ai.meta.com/llama) [7B](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML)
    examples:
    - What is the capital of the Netherlands?
    - Which drugs are approved by the FDA to mitigate Alzheimer symptoms?
    - What was the GDP of France in 1998?
    favicon: https://raw.github.com/vemonet/libre-chat/main/docs/docs/assets/logo.png
    repository_url: https://github.com/vemonet/libre-chat
    public_url: https://chat.semanticscience.org
    contact:
      name: "Vincent Emonet"
      email: "vincent.emonet@gmail.com"
    license_info:
      name: "MIT license"
      url: "https://raw.github.com/vemonet/libre-chat/main/LICENSE.txt"
    workers: 4

# Password to access the admin API
password: ""


image:
  repository: ghcr.io/maastrichtu-ids/libre-chat
  tag: "main"
  pullPolicy: Always
  # or IfNotPresent
  command: []
  # command: "uvicorn main:app --reload"

storage:
  enabled: true
  size: 10Gi
  mountPath: /data
  # Let workspace empty to use the mountPath as workspace
  workingDir: /data
  # Mount the distributed shared memory
  enableDshm: true
  extraStorage: []
  # extraStorage:
  #   - name: easybuild-data
  #     mountPath: /opt/apps/easybuild
  #     readOnly: true

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  name: "anyuid"
  # Annotations to add to the service account
  annotations: {}

service:
  port: 8000
  type: ClusterIP
  openshiftRoute:
    enabled: true
    host: ""
    path: ""
    wildcardPolicy: None
    tls:
      enabled: true
      termination: edge
      insecureEdgeTerminationPolicy: Redirect
  ingress:
    enabled: false
    annotations: {}
    hosts:
      - host: chart-example.local
        paths: []
    tls: []

extraEnvs: []
# extraEnvs:
#   - name: PASSWORD
#     value: mypassword

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
  # Assign a GPU to this JupyterLab (requires to have GPU quota in the project):
  # requests:
  #   nvidia.com/gpu: '1'
  # limits:
  #   nvidia.com/gpu: '1'

tolerations: []
## For GPU use:
# tolerations:
#   - effect: NoSchedule
#     key: "nvidia.com/gpu"
#     operator: Exists


securityContext: {}
  # runAsUser: 0
  # supplementalGroups:
  # - 100
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

podSecurityContext: {}
  # runAsUser: 0
  # supplementalGroups:
  # - 100
imagePullSecrets: []
podAnnotations: {}
nodeSelector: {}
affinity: {}
